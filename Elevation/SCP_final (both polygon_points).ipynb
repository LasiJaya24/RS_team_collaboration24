{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "066ffbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `verify_cert` to False is a security risk, use at your own risk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lidar coverage union created successfully\n",
      "LiDAR union saved to C:\\projects\\New_Script_SCP\\points\\outputs_3\\Lidar_Union.shp\n",
      "Checking water storage 1...\n",
      "Polygon 1 is fully covered by LiDAR.\n",
      "Point geometry detected for water storage 1. Using 5000m buffer.\n",
      "Tile 1 processed successfully\n",
      "Clipped raster saved: C:\\projects\\temp_storage\\tmp21n2zo7u\\Clipped_Tile_1_0.tif\n",
      "Tile 2 processed successfully\n",
      "Clipped raster saved: C:\\projects\\temp_storage\\tmp21n2zo7u\\Clipped_Tile_1_1.tif\n",
      "Tile 3 processed successfully\n",
      "Clipped raster saved: C:\\projects\\temp_storage\\tmp21n2zo7u\\Clipped_Tile_1_2.tif\n",
      "Tile 4 processed successfully\n",
      "Clipped raster saved: C:\\projects\\temp_storage\\tmp21n2zo7u\\Clipped_Tile_1_3.tif\n",
      "Tile 5 processed successfully\n",
      "Clipped raster saved: C:\\projects\\temp_storage\\tmp21n2zo7u\\Clipped_Tile_1_4.tif\n",
      "Tile 6 processed successfully\n",
      "Clipped raster saved: C:\\projects\\temp_storage\\tmp21n2zo7u\\Clipped_Tile_1_5.tif\n",
      "Tile 7 processed successfully\n",
      "Clipped raster saved: C:\\projects\\temp_storage\\tmp21n2zo7u\\Clipped_Tile_1_6.tif\n",
      "Tile 8 processed successfully\n",
      "Clipped raster saved: C:\\projects\\temp_storage\\tmp21n2zo7u\\Clipped_Tile_1_7.tif\n",
      "Tile 9 processed successfully\n",
      "Clipped raster saved: C:\\projects\\temp_storage\\tmp21n2zo7u\\Clipped_Tile_1_8.tif\n",
      "Tile 10 processed successfully\n",
      "Clipped raster saved: C:\\projects\\temp_storage\\tmp21n2zo7u\\Clipped_Tile_1_9.tif\n",
      "Tile 11 processed successfully\n",
      "Clipped raster saved: C:\\projects\\temp_storage\\tmp21n2zo7u\\Clipped_Tile_1_10.tif\n",
      "Tile 12 processed successfully\n",
      "Clipped raster saved: C:\\projects\\temp_storage\\tmp21n2zo7u\\Clipped_Tile_1_11.tif\n",
      "Tile 13 processed successfully\n",
      "Clipped raster saved: C:\\projects\\temp_storage\\tmp21n2zo7u\\Clipped_Tile_1_12.tif\n",
      "Tile 14 processed successfully\n",
      "Clipped raster saved: C:\\projects\\temp_storage\\tmp21n2zo7u\\Clipped_Tile_1_13.tif\n",
      "Tile 15 processed successfully\n",
      "Clipped raster saved: C:\\projects\\temp_storage\\tmp21n2zo7u\\Clipped_Tile_1_14.tif\n",
      "Tile 16 processed successfully\n",
      "Clipped raster saved: C:\\projects\\temp_storage\\tmp21n2zo7u\\Clipped_Tile_1_15.tif\n",
      "Merged DEM saved to C:\\projects\\temp_storage\\tmp21n2zo7u\\Merged_DEM_1.tif\n",
      "Contours generated and saved to C:\\projects\\temp_storage\\tmp21n2zo7u\\Contours_1.shp\n",
      "No valid contour found covering the centroid.\n",
      "Contours generated and saved to C:\\projects\\temp_storage\\tmp21n2zo7u\\Contours_1.shp\n",
      "No valid contour found covering the centroid.\n",
      "Deleted temporary directory: C:\\projects\\temp_storage\\tmp21n2zo7u\n",
      "LiDAR status with SCP status saved to C:\\projects\\New_Script_SCP\\points\\outputs_3\\lidar_status.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.raster import ImageryLayer\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box, Point, Polygon, LineString\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.merge import merge\n",
    "from rasterio.io import MemoryFile\n",
    "import subprocess\n",
    "import pandas as pd  \n",
    "import sys\n",
    "from shapely.ops import unary_union\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "    \n",
    "def create_tiles(extent, tile_size):\n",
    "    xmin, ymin, xmax, ymax = extent\n",
    "    xstep, ystep = tile_size\n",
    "    tiles = [(x, y, x + xstep, y + ystep) for x in range(int(xmin), int(xmax), xstep) for y in range(int(ymin), int(ymax), ystep)]\n",
    "    return tiles\n",
    "\n",
    "def clip_raster_with_geometry(input_raster, geometry, output_raster):\n",
    "    with rasterio.open(input_raster) as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(src, [geometry], crop=True)\n",
    "        out_meta = src.meta.copy()\n",
    "    \n",
    "    # Update metadata outside the 'with' block\n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": out_image.shape[1],\n",
    "        \"width\": out_image.shape[2],\n",
    "        \"transform\": out_transform\n",
    "    })\n",
    "    \n",
    "    with rasterio.open(output_raster, \"w\", **out_meta) as dest:\n",
    "        dest.write(out_image)\n",
    "    \n",
    "    print(f\"Clipped raster saved: {output_raster}\")\n",
    "\n",
    "    \n",
    "\n",
    "def process_tile(secure_img_lyr, portal, xmin, ymin, xmax, ymax, i, temp_dir):\n",
    "    try:\n",
    "        export_result = secure_img_lyr.export_image(\n",
    "            bbox=[xmin, ymin, xmax, ymax],\n",
    "            image_sr=3857,\n",
    "            size=[1024, 1024],\n",
    "            export_format='tiff'\n",
    "        )\n",
    "        dem_url = export_result['href']\n",
    "        token = portal._con.token\n",
    "        dem_url_with_token = f\"{dem_url}&token={token}\"\n",
    "        response = requests.get(dem_url_with_token)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        memfile = MemoryFile(response.content)\n",
    "        dataset = memfile.open()\n",
    "        tile_path = os.path.join(temp_dir, f\"Tile_{i}.tif\")\n",
    "        dataset.meta.update({\"driver\": \"GTiff\"})\n",
    "        with rasterio.open(tile_path, 'w', **dataset.meta) as dest:\n",
    "            dest.write(dataset.read(1), 1)\n",
    "        print(f\"Tile {i + 1} processed successfully\")\n",
    "        return tile_path\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"HTTP request error for Tile {i}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing DEM for Tile {i}: {e}\")\n",
    "    return None\n",
    "\n",
    "def generate_and_analyze_contours(dem_path, centroid, output_contour_path, output_max_contour_path):\n",
    "    contour_interval = 0.1\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"gdal_contour\",\n",
    "            \"-i\", str(contour_interval),\n",
    "            \"-a\", \"ELEV\",\n",
    "            dem_path,\n",
    "            output_contour_path\n",
    "        ], check=True, capture_output=True, text=True)\n",
    "        print(f\"Contours generated and saved to {output_contour_path}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error generating contours:\")\n",
    "        print(e.stderr)\n",
    "        print(e.stdout)\n",
    "        return\n",
    "\n",
    "    # Load the contours as a GeoDataFrame\n",
    "    contours_gdf = gpd.read_file(output_contour_path)\n",
    "    if contours_gdf.crs is None:\n",
    "        contours_gdf.set_crs(epsg=3857, allow_override=True, inplace=True)\n",
    "\n",
    "    max_contour = None\n",
    "    max_elevation = float('-inf')\n",
    "\n",
    "    # Define a helper function to convert LineString to Polygon\n",
    "    def convert_to_polygon(geometry):\n",
    "        if isinstance(geometry, LineString) and geometry.is_ring:\n",
    "            return Polygon(geometry)\n",
    "        return None\n",
    "\n",
    "    # Add a column for polygons\n",
    "    contours_gdf['polygon'] = contours_gdf.geometry.apply(convert_to_polygon)\n",
    "\n",
    "    # Add a column for distance to centroid\n",
    "    contours_gdf['distance_to_centroid'] = contours_gdf.geometry.distance(centroid)\n",
    "\n",
    "    # Sort the contours by distance to the centroid\n",
    "    sorted_contours = contours_gdf.sort_values(by='distance_to_centroid')\n",
    "\n",
    "    # Evaluate contours one by one\n",
    "    for _, row in sorted_contours.iterrows():\n",
    "        polygon = row['polygon']\n",
    "        if polygon and polygon.is_valid:\n",
    "            if polygon.contains(centroid):\n",
    "                # Update the max contour if the elevation is higher\n",
    "                if row['ELEV'] > max_elevation:\n",
    "                    max_contour = polygon\n",
    "                    max_elevation = row['ELEV']\n",
    "\n",
    "    # Save the maximum contour polygon\n",
    "    if max_contour:\n",
    "        max_contour_gdf = gpd.GeoDataFrame(\n",
    "            [{'geometry': max_contour, 'full_suppl': max_elevation}],\n",
    "            crs='EPSG:3857'\n",
    "        )\n",
    "        max_contour_gdf.to_file(output_max_contour_path)\n",
    "        print(f\"Maximum contour polygon saved at {output_max_contour_path}\")\n",
    "    else:\n",
    "        print(\"No valid contour found covering the centroid.\")\n",
    "    # **Ensure the GeoDataFrame is fully closed**\n",
    "    del contours_gdf\n",
    "\n",
    "    \n",
    "\n",
    "def combine_max_contour_polygons(output_folder, original_shapefile, LidarZone_project_shapefile):\n",
    "    max_contour_files = [f for f in os.listdir(output_folder) if f.startswith(\"MaxContourPolygon_\") and f.endswith(\".shp\")]\n",
    "    all_max_contours = []\n",
    "\n",
    "    for contour_file in max_contour_files:\n",
    "        contour_path = os.path.join(output_folder, contour_file)\n",
    "        try:\n",
    "            contour_gdf = gpd.read_file(contour_path)\n",
    "            all_max_contours.append(contour_gdf)\n",
    "            print(f\"Added {contour_file} to the combined shapefile list.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {contour_file}: {e}\")\n",
    "    if all_max_contours:\n",
    "        combined_max_contour_gdf = gpd.GeoDataFrame(pd.concat(all_max_contours, ignore_index=True), crs=all_max_contours[0].crs)\n",
    "\n",
    "        # Convert to Albers Equal Area projection for area calculation\n",
    "        combined_max_contour_gdf = combined_max_contour_gdf.to_crs(epsg=3577)\n",
    "\n",
    "        # Read original shapefile and ensure CRS matches\n",
    "        try:\n",
    "            original_gdf = gpd.read_file(original_shapefile)\n",
    "            original_gdf = original_gdf.to_crs(combined_max_contour_gdf.crs)\n",
    "\n",
    "            # Perform spatial join to associate PFI values\n",
    "            combined_max_contour_gdf = gpd.sjoin(combined_max_contour_gdf, original_gdf[['geometry', 'pfi', 'ufi', 'origin', 'function','alternate_', 'qld_pndb_i', 'additional', 'add_names_', 'perenniali', 'hierarchy', 'drainage_b', 'qld_wtr_st', 'constructe', 'volume_ml', 'upper_scal', 'text_note','add_inform', 'feature_ty', 'name', 'attribute_',  'dimension_', 'globalid', 'feature_so']], predicate='intersects')\n",
    "            # Rename index_right after spatial join to avoid conflict\n",
    "            if 'index_right' in combined_max_contour_gdf.columns:\n",
    "                combined_max_contour_gdf.rename(columns={'index_right': 'original_index'}, inplace=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or joining original shapefile: {e}\")\n",
    "            return\n",
    "\n",
    "        # Read the lidar_zone shapefile containing lidar project names\n",
    "        \n",
    "        try:\n",
    "            # Read the lidar zone shapefile\n",
    "            lidar_zone_gdf = gpd.read_file(LidarZone_project_shapefile)\n",
    "            lidar_zone_gdf = lidar_zone_gdf.to_crs(combined_max_contour_gdf.crs)\n",
    "\n",
    "            # Rename 'Name' field to 'feature_source'\n",
    "            lidar_zone_gdf.rename(columns={'Name': 'lidar_source'}, inplace=True)\n",
    "\n",
    "            # Ensure lidar_zone_gdf is sorted by lidar_year in descending order to prioritize the most recent year\n",
    "            lidar_zone_gdf = lidar_zone_gdf.sort_values(by='lidar_year', ascending=False)\n",
    "\n",
    "            # Perform spatial join with lidar zone to get lidar_project name\n",
    "            combined_max_contour_gdf = gpd.sjoin(combined_max_contour_gdf, lidar_zone_gdf[['geometry', 'lidar_source', 'lidar_year']], predicate='intersects')\n",
    "\n",
    "            # Drop duplicate feature_sources within the same polygon based on lidar_year (most recent will remain)\n",
    "            combined_max_contour_gdf = combined_max_contour_gdf.drop_duplicates(subset='geometry', keep='first')\n",
    "\n",
    "            # Rename the index_right column after the spatial join to avoid conflict\n",
    "            if 'index_right' in combined_max_contour_gdf.columns:\n",
    "                combined_max_contour_gdf.rename(columns={'index_right': 'lidar_project_index'}, inplace=True)            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or joining lidar zone shapefile: {e}\")\n",
    "\n",
    "\n",
    "        # Calculate area and add latitude/longitude fields\n",
    "        combined_max_contour_gdf['Area_sqm'] = combined_max_contour_gdf.geometry.area\n",
    "        combined_max_contour_gdf['Perimeter_m'] = combined_max_contour_gdf.geometry.boundary.length\n",
    "        combined_max_contour_gdf['Lon'] = combined_max_contour_gdf.geometry.centroid.x\n",
    "        combined_max_contour_gdf['Lat'] = combined_max_contour_gdf.geometry.centroid.y\n",
    "        run_date = datetime.today().strftime('%y-%m-%d')\n",
    "        combined_max_contour_gdf['run_date'] = run_date\n",
    "\n",
    "        combined_max_contour_path = os.path.join(output_folder, \"Combined_MaxContourPolygons.shp\")\n",
    "\n",
    "        # Rename columns to shorter names for shapefile compatibility\n",
    "        combined_max_contour_gdf.rename(columns={\n",
    "            'Area_sqm': 'AreaSqm',\n",
    "            'Perimeter_m': 'Peri_m',\n",
    "            'Lon': 'Longitude',\n",
    "            'Lat': 'Latitude',\n",
    "            'original_index': 'orig_idx',  # Renamed for compatibility\n",
    "            'lidar_project_index': 'lidar_proj',\n",
    "            'lidar_source': 'lidar_so'\n",
    "            # Renamed for compatibility\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Save the combined GeoDataFrame to a shapefile\n",
    "        if not combined_max_contour_gdf.empty:\n",
    "            combined_max_contour_gdf.to_file(combined_max_contour_path)\n",
    "            print(f\"All MaxContourPolygons have been combined and saved to {combined_max_contour_path}\")\n",
    "        else:\n",
    "            print(\"No MaxContourPolygons found to combine.\")\n",
    "            \n",
    "\n",
    "def run_combined_script():\n",
    "    portal = GIS(url='https://auth-spatial.information.qld.gov.au/arcgis/', username='', password='', verify_cert=False)\n",
    "    secure_url = 'https://auth-spatial-img.information.qld.gov.au/arcgis/rest/services/Elevation/DEM_TimeSeries_CLPUsers/ImageServer'\n",
    "    secure_img_lyr = ImageryLayer(secure_url, portal)\n",
    "    output_folder = r'C:\\projects\\New_Script_SCP\\points\\outputs_3'\n",
    "    base_temp_dir = r\"C:\\projects\\temp_storage\"\n",
    "    tile_size = (1000, 1000)\n",
    "    \n",
    "    # List to store polygon ID and lidar status\n",
    "    lidar_status_list = []\n",
    "    \n",
    "    shapefile_path = r'C:\\projects\\New_Script_SCP\\points\\inputs\\point4.shp'\n",
    "    lidar_coverage_path = r'C:\\projects\\Lidar_zone\\LidarZone_project.shp'\n",
    "\n",
    "    # Load the shapefile\n",
    "    try:\n",
    "        gdf = gpd.read_file(shapefile_path)\n",
    "        if gdf.empty:\n",
    "            print(\"Error: Shapefile is empty.\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading shapefile: {e}\")\n",
    "        return\n",
    "\n",
    "    gdf = gdf.to_crs(epsg=3857)  # Ensure CRS is EPSG:3857\n",
    "\n",
    "    # Load LiDAR coverage\n",
    "    try:\n",
    "        lidar_gdf = gpd.read_file(lidar_coverage_path).to_crs(epsg=3857)\n",
    "        lidar_union = unary_union(lidar_gdf.geometry)  # Combine all lidar zones into one geometry\n",
    "        print(\"Lidar coverage union created successfully\")\n",
    "        lidar_union_gdf = gpd.GeoDataFrame(geometry=[lidar_union], crs=lidar_gdf.crs)\n",
    "        lidar_union_output_path = os.path.join(output_folder, \"Lidar_Union.shp\")\n",
    "        lidar_union_gdf.to_file(lidar_union_output_path)\n",
    "        print(f\"LiDAR union saved to {lidar_union_output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading LiDAR coverage: {e}\")\n",
    "        return\n",
    "\n",
    "    # Process each polygon\n",
    "    for idx, polygon in gdf.iterrows():\n",
    "        print(f\"Checking water storage {idx + 1}...\")\n",
    "\n",
    "        # Create a temporary directory for each polygon\n",
    "        temp_dir = tempfile.mkdtemp(dir=base_temp_dir)\n",
    "\n",
    "        # Extract pfi and ufi values from the shapefile\n",
    "        pfi = polygon.get('pfi', None)\n",
    "        ufi = polygon.get('ufi', None)\n",
    "\n",
    "        # Initialize scp_status to \"no\" by default\n",
    "        scp_status = \"no\"\n",
    "\n",
    "        # Check if polygon is completely within LiDAR coverage\n",
    "        if polygon.geometry.within(lidar_union):\n",
    "            lidar_status = \"yes\"\n",
    "            print(f\"Polygon {idx + 1} is fully covered by LiDAR.\")\n",
    "        else:\n",
    "            lidar_status = \"no\"\n",
    "            print(f\"Polygon {idx + 1} is NOT fully covered by LiDAR.\")\n",
    "\n",
    "        # Append the polygon ID, pfi, ufi, lidar status to the list\n",
    "        lidar_status_list.append({\n",
    "            \"polygon_id\": idx + 1,\n",
    "            \"pfi\": pfi,\n",
    "            \"ufi\": ufi,\n",
    "            \"lidar_status\": lidar_status,\n",
    "            \"scp_status\": scp_status  # Add scp_status to the list\n",
    "        })\n",
    "\n",
    "        # Continue processing only if fully covered\n",
    "        if lidar_status == \"yes\":\n",
    "            # Check if the geometry is a point or polygon and assign buffer size accordingly\n",
    "            if isinstance(polygon.geometry, Point):\n",
    "                buffer_distance = 1000  # Use 1000m buffer for points\n",
    "                print(f\"Point geometry detected for water storage {idx + 1}. Using 5000m buffer.\")\n",
    "            else:\n",
    "                buffer_distance = 50  # Use 50m buffer for polygons\n",
    "                print(f\"Polygon geometry detected for water storage {idx + 1}. Using 50m buffer.\")\n",
    "\n",
    "            # Create a buffered geometry\n",
    "            gdf_buffered = polygon.geometry.buffer(buffer_distance)\n",
    "            buffered_extent = gdf_buffered.bounds\n",
    "            tiles = create_tiles(buffered_extent, tile_size)\n",
    "            dem_tiles = []\n",
    "\n",
    "            for i, (xmin, ymin, xmax, ymax) in enumerate(tiles):\n",
    "                tile_path = process_tile(secure_img_lyr, portal, xmin, ymin, xmax, ymax, i, temp_dir)\n",
    "                if tile_path:\n",
    "                    clipped_tile_path = os.path.join(temp_dir, f\"Clipped_Tile_{idx + 1}_{i}.tif\")\n",
    "                    clip_raster_with_geometry(tile_path, gdf_buffered, clipped_tile_path)\n",
    "                    dem_tiles.append(clipped_tile_path)\n",
    "\n",
    "            # Open each DEM tile outside the 'with' block to ensure they stay open during the merge\n",
    "            src_files_to_mosaic = []\n",
    "            for tile in dem_tiles:\n",
    "                try:\n",
    "                    src = rasterio.open(tile)\n",
    "                    src_files_to_mosaic.append(src)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error opening tile {tile}: {e}\")\n",
    "\n",
    "            # Now use src_files_to_mosaic to merge\n",
    "            if src_files_to_mosaic:\n",
    "                try:\n",
    "                    mosaic, out_transform = merge(src_files_to_mosaic)\n",
    "                    mosaic = mosaic.squeeze()\n",
    "\n",
    "                    # Define the output mosaic path\n",
    "                    output_mosaic_path = os.path.join(temp_dir, f\"Merged_DEM_{idx + 1}.tif\")\n",
    "\n",
    "                    # Write the merged DEM to a file\n",
    "                    with rasterio.open(output_mosaic_path, 'w', driver='GTiff',\n",
    "                                       height=mosaic.shape[0], width=mosaic.shape[1], \n",
    "                                       count=1, dtype=mosaic.dtype,\n",
    "                                       crs='EPSG:3857', transform=out_transform) as dst:\n",
    "                        dst.write(mosaic, 1)\n",
    "\n",
    "                    print(f\"Merged DEM saved to {output_mosaic_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during merging tiles: {e}\")\n",
    "                finally:\n",
    "                    # Ensure files are closed after processing\n",
    "                    for src in src_files_to_mosaic:\n",
    "                        src.close()\n",
    "\n",
    "                    # Generate contours and analyze max contour polygon only if the mosaic was successfully created\n",
    "                    centroid = polygon.geometry.representative_point()\n",
    "                    output_contour_path = os.path.join(temp_dir, f\"Contours_{idx + 1}.shp\")\n",
    "                    output_max_contour_path = os.path.join(output_folder, f\"MaxContourPolygon_{idx + 1}.shp\")\n",
    "                    generate_and_analyze_contours(output_mosaic_path, centroid, output_contour_path, output_max_contour_path)\n",
    "                    \n",
    "                try:    \n",
    "                    # Your main code here\n",
    "                    if os.path.exists(output_mosaic_path):\n",
    "                        try:\n",
    "                            \n",
    "                            generate_and_analyze_contours(output_mosaic_path, centroid, output_contour_path, output_max_contour_path)\n",
    "\n",
    "                            # Check if a max contour polygon was found\n",
    "                            if os.path.exists(output_max_contour_path):\n",
    "                                scp_status = \"yes\"  # If max contour polygon exists, update status to 'yes'\n",
    "                            else:\n",
    "                                scp_status = \"no\"  # If not found, keep status as 'no'\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error during contour generation and analysis: {e}\")\n",
    "                    else:\n",
    "                        print(f\"Mosaic DEM not found for polygon {idx + 1}, skipping contour generation.\")\n",
    "                except Exception as e:\n",
    "                     print(f\"Error during merging or contour analysis for storage {idx + 1}: {e}\")\n",
    "                \n",
    "\n",
    "            # Delete the intermediate files after processing (after closing them)\n",
    "                def safe_delete(filepath):\n",
    "                    try:\n",
    "                        if os.path.exists(filepath):\n",
    "                            os.remove(filepath)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error deleting {filepath}: {e}\")\n",
    "\n",
    "                # Delete individual tiles\n",
    "                for tile in dem_tiles:\n",
    "                    safe_delete(tile)\n",
    "\n",
    "                # Delete merged DEM\n",
    "                safe_delete(output_mosaic_path)\n",
    "\n",
    "                # Delete contour and max contour shapefiles\n",
    "                safe_delete(output_contour_path)\n",
    "                \n",
    "\n",
    "                # Delete temp directory\n",
    "                try:\n",
    "                    shutil.rmtree(temp_dir)\n",
    "                    print(f\"Deleted temporary directory: {temp_dir}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error deleting {temp_dir}: {e}\")\n",
    "                    \n",
    "                #except Exception as e:\n",
    "                    #print(f\"Error during merging or contour analysis for storage {idx + 1}: {e}\")\n",
    "\n",
    "        # Update the scp_status in the lidar_status_list\n",
    "        lidar_status_list[-1][\"scp_status\"] = scp_status\n",
    "\n",
    "    # Create DataFrame from lidar_status_list\n",
    "    lidar_status_df = pd.DataFrame(lidar_status_list)\n",
    "    \n",
    "    # Combine all MaxContourPolygons into one shapefile\n",
    "    combine_max_contour_polygons(\n",
    "        output_folder=output_folder,\n",
    "        original_shapefile=shapefile_path,\n",
    "        LidarZone_project_shapefile=lidar_coverage_path    \n",
    "    )   \n",
    "\n",
    "    # Save the updated CSV\n",
    "    lidar_status_csv_path = os.path.join(output_folder, \"lidar_status.csv\")\n",
    "    lidar_status_df.to_csv(lidar_status_csv_path, index=False)\n",
    "    print(f\"LiDAR status with SCP status saved to {lidar_status_csv_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_combined_script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e3af0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
